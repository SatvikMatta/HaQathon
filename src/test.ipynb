{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5de6dff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python arch : ARM64\n",
      "ONNX RT     : 1.22.0\n",
      "Providers   : ['QNNExecutionProvider', 'CPUExecutionProvider']\n",
      "📸  Capturing screenshot …\n",
      "✅  Screenshot captured.\n",
      "\n",
      "🧠  Classification result\n",
      "   → label        : a code editor\n",
      "   → score (×100) : 31.50\n",
      "   → probability  : 15.42%\n",
      "\n",
      "🔍  Full table\n",
      "   - a web browser       :  25.39   (14.51%)\n",
      "   - a terminal          :  27.00   (14.74%)\n",
      "   - a YouTube video     :  19.73   (13.71%)\n",
      "   - a code editor       :  31.50   (15.42%)\n",
      "   - a game              :  20.45   (13.81%)\n",
      "   - a cat               :  20.28   (13.79%)\n",
      "   - a spreadsheet       :  21.97   (14.02%)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "CLIP – NPU demo (no torch, no qai-hub-models)\n",
    "Screenshot → QNN → similarity table\n",
    "\"\"\"\n",
    "from __future__ import annotations\n",
    "from pathlib import Path\n",
    "import platform, time\n",
    "\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "from PIL import ImageGrab\n",
    "import onnxruntime as ort\n",
    "from transformers import CLIPTokenizer\n",
    "\n",
    "# ── 0.  Environment info ────────────────────────────────────────────────\n",
    "print(\"Python arch :\", platform.machine())          # arm64 or AMD64\n",
    "print(\"ONNX RT     :\", ort.__version__)\n",
    "\n",
    "# ── 1.  Build QNN session ───────────────────────────────────────────────\n",
    "onnx_path = Path(\"openai_clip.onnx\")                 # QDQ / quantised!\n",
    "assert onnx_path.exists(), f\"Missing {onnx_path}\"\n",
    "\n",
    "ort_dir  = Path(ort.__file__).parent\n",
    "qnn_dll  = ort_dir / \"capi\" / \"QnnHtp.dll\"\n",
    "\n",
    "sess = ort.InferenceSession(\n",
    "    onnx_path,\n",
    "    providers=[(\"QNNExecutionProvider\", {\"backend_path\": str(qnn_dll)}),\n",
    "               \"CPUExecutionProvider\"]\n",
    ")\n",
    "print(\"Providers   :\", sess.get_providers())\n",
    "\n",
    "img_name = sess.get_inputs()[0].name                 # \"images\"\n",
    "txt_name = sess.get_inputs()[1].name                 # \"texts\"\n",
    "out_name = sess.get_outputs()[0].name                # \"similarities\"\n",
    "\n",
    "# ── 2.  Tokeniser (pure Python) ─────────────────────────────────────────\n",
    "tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch16\")\n",
    "\n",
    "def encode_text(prompt: str) -> np.ndarray:\n",
    "    \"\"\"Return a (1,77) int32 tensor for CLIP.\"\"\"\n",
    "    ids = tokenizer(prompt,\n",
    "                    padding=\"max_length\",\n",
    "                    truncation=True,\n",
    "                    max_length=77,\n",
    "                    return_tensors=\"np\").input_ids\n",
    "    return ids.astype(np.int32)\n",
    "\n",
    "# ── 3.  Image pre-processing (OpenCV/NumPy) ─────────────────────────────\n",
    "def preprocess_image(pil_img) -> np.ndarray:\n",
    "    \"\"\"PIL → (1,3,224,224) float32, NCHW, normalized.\"\"\"\n",
    "    img = cv.cvtColor(np.array(pil_img), cv.COLOR_RGB2BGR)\n",
    "    img = cv.resize(img, (224, 224), interpolation=cv.INTER_CUBIC)\n",
    "    img = img[:, :, ::-1]  # BGR→RGB\n",
    "    img = img.astype(np.float32) / 255.0\n",
    "    img = (img - [0.485,0.456,0.406]) / [0.229,0.224,0.225]\n",
    "    chw  = np.transpose(img, (2,0,1))[None]          # (1,3,224,224)\n",
    "    return chw.astype(np.float32)\n",
    "\n",
    "# ── 4.  Capture screen once ─────────────────────────────────────────────\n",
    "print(\"📸  Capturing screenshot …\")\n",
    "time.sleep(1)\n",
    "pil_shot = ImageGrab.grab()\n",
    "print(\"✅  Screenshot captured.\")\n",
    "\n",
    "img_tensor = preprocess_image(pil_shot)\n",
    "\n",
    "# ── 5.  Label list  – edit freely ───────────────────────────────────────\n",
    "labels = [\n",
    "    \"a web browser\", \"a terminal\", \"a YouTube video\", \"a game\", \"a cat\", \"a spreadsheet\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e94a2a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🧠  Classification result\n",
      "   → label        : a code editor\n",
      "   → score (×100) : 31.50\n",
      "   → probability  : 15.42%\n",
      "\n",
      "🔍  Full table\n",
      "   - a web browser       :  25.39   (14.51%)\n",
      "   - a terminal          :  27.00   (14.74%)\n",
      "   - a YouTube video     :  19.73   (13.71%)\n",
      "   - a code editor       :  31.50   (15.42%)\n",
      "   - a game              :  20.45   (13.81%)\n",
      "   - a cat               :  20.28   (13.79%)\n",
      "   - a spreadsheet       :  21.97   (14.02%)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "scores = []\n",
    "for lbl in labels:\n",
    "    txt_tensor = encode_text(lbl)\n",
    "    sim = sess.run([out_name],\n",
    "                   {img_name: img_tensor,\n",
    "                    txt_name: txt_tensor})[0]        # (1,1) ×100\n",
    "    scores.append(float(sim.squeeze()))\n",
    "\n",
    "scores = np.array(scores)\n",
    "probs  = np.exp(scores/100 - scores.max()/100)\n",
    "probs /= probs.sum()\n",
    "\n",
    "best = int(scores.argmax())\n",
    "print(\"\\n🧠  Classification result\")\n",
    "print(f\"   → label        : {labels[best]}\")\n",
    "print(f\"   → score (×100) : {scores[best]:.2f}\")\n",
    "print(f\"   → probability  : {probs[best]:.2%}\")\n",
    "\n",
    "print(\"\\n🔍  Full table\")\n",
    "for lbl, sc, pb in zip(labels, scores, probs):\n",
    "    print(f\"   - {lbl:20s}: {sc:6.2f}   ({pb:.2%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b74816a8",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "from pathlib import Path\n",
    "import platform, time\n",
    "\n",
    "import cv2 as cv\n",
    "# import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc4a16a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenCV 4.12.0 on ARM64\n"
     ]
    }
   ],
   "source": [
    "import os   # standard library\n",
    "\n",
    "# 0 = keep OpenCV’s baseline-feature guard ON\n",
    "# 1 = skip the guard entirely\n",
    "os.environ[\"OPENCV_SKIP_CPU_BASELINE_CHECK\"] = \"1\"\n",
    "\n",
    "from pathlib import Path\n",
    "import platform, time\n",
    "import cv2 as cv          # safe to import now\n",
    "\n",
    "print(\"OpenCV\", cv.getVersionString(), \"on\", platform.machine())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e338ad5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
