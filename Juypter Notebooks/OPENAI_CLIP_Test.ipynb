{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ff54b470",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------\n",
    "# Copyright (c) 2024 Qualcomm Innovation Center, Inc. All rights reserved.\n",
    "# SPDX-License-Identifier: BSD-3-Clause\n",
    "# ---------------------------------------------------------------------\n",
    "from __future__ import annotations\n",
    "\n",
    "from collections.abc import Sequence\n",
    "from pathlib import Path\n",
    "from typing import Callable\n",
    "\n",
    "import torch\n",
    "from PIL.Image import Image\n",
    "\n",
    "from qai_hub_models.models.protocols import ExecutableModelProtocol\n",
    "from qai_hub_models.utils.asset_loaders import load_image\n",
    "\n",
    "\n",
    "class ClipApp:\n",
    "    \"\"\"\n",
    "    This class consists of light-weight \"app code\" that is required to perform end to end inference with Clip.\n",
    "\n",
    "    The app uses 1 model:\n",
    "        * Clip\n",
    "\n",
    "    For a given image input, the app will:\n",
    "        * pre-process the image\n",
    "        * pre-process the text\n",
    "        * Run Clip inference\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        # Model has two inputs:\n",
    "        #  - image (N, 3, H, W), RGB, float[0:1]\n",
    "        #  - tokenized text (N, 77)\n",
    "        model: ExecutableModelProtocol[torch.Tensor],\n",
    "        text_tokenizer: Callable[[str], torch.Tensor],\n",
    "        image_preprocessor: Callable[[Image], torch.Tensor],\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.text_tokenizer = text_tokenizer\n",
    "        self.image_preprocessor = image_preprocessor\n",
    "\n",
    "    def predict(self, *args, **kwargs):\n",
    "        # See predict_similarity.\n",
    "        return self.predict_similarity(*args, **kwargs)\n",
    "\n",
    "    def predict_similarity(\n",
    "        self, images_or_image_paths: Sequence[Image | str | Path], texts: Sequence[str]\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            images_or_image_paths: PIL Image or path to an image file / URL.\n",
    "            texts: String texts to search for similarity.\n",
    "\n",
    "        Outputs:\n",
    "            cosine_similarities_per_image: torch.Tensor (Shape: [num_images, num_text_prompts])\n",
    "                Given a batch of images and a batch of text tokens, returns a tensor,\n",
    "                containing the cosine similarity scores corresponding to each image per text input.\n",
    "                The values are cosine similarities between the corresponding image and\n",
    "                text features, times 100. The cosine similarities of text per image can be computed\n",
    "                by doing a transpose.\n",
    "        \"\"\"\n",
    "        preprocessed_images: list[torch.Tensor] = []\n",
    "\n",
    "        # Process each image to be a tensor  of shape [NImages, 3, 224, 224] with layout RGB and range [0 - 1 ]\n",
    "        for image_or_path in images_or_image_paths:\n",
    "            if isinstance(image_or_path, str) or isinstance(image_or_path, Path):\n",
    "                image_or_path = load_image(image_or_path)\n",
    "            preprocessed_images.append(self.image_preprocessor(image_or_path))\n",
    "        preprocessed_stacked_images = torch.stack(preprocessed_images)\n",
    "\n",
    "        # Tokenize string text to shape [NTexts, 77]\n",
    "        preprocessed_texts: list[torch.Tensor] = [self.text_tokenizer(x) for x in texts]\n",
    "        preprocessed_stacked_texts = torch.cat(preprocessed_texts)\n",
    "\n",
    "        return self.model(preprocessed_stacked_images, preprocessed_stacked_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d907492f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Single Image Classification ===\n",
      "Predicted class: games\n",
      "Confidence score: 92.04\n",
      "Probability: 51.76%\n",
      "\n",
      "=== All Class Scores ===\n",
      "code: 13.02 (probability: 23.49%)\n",
      "games: 92.04 (probability: 51.76%)\n",
      "video: 18.29 (probability: 24.76%)\n",
      "\n",
      "=== Top 3 Predictions ===\n",
      "1. code: 27.41 (46.55%)\n",
      "2. video: 4.04 (36.85%)\n",
      "3. games: -75.75 (16.59%)\n",
      "\n",
      "=== Multiple Images Classification ===\n",
      "Image: C:\\Users\\qc_de\\Documents\\HaQathon\\anythingLLMtests\\screenshot.png\n",
      "Predicted: games (59.52%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Dict\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Enhanced ClipApp with classification methods\n",
    "class ClipClassifier:\n",
    "    def __init__(self, clip_app: ClipApp):\n",
    "        self.clip_app = clip_app\n",
    "    \n",
    "    def classify_single_image(self, image_path: str, class_labels: List[str]) -> Dict:\n",
    "        \"\"\"\n",
    "        Classify a single image against multiple text labels\n",
    "        Returns the most likely class with confidence scores\n",
    "        \"\"\"\n",
    "        similarities = self.clip_app.predict_similarity([image_path], class_labels)\n",
    "        \n",
    "        # Get probabilities using softmax\n",
    "        probabilities = F.softmax(similarities[0] / 100.0, dim=0)  # Divide by 100 since similarities are scaled\n",
    "        \n",
    "        # Get the best match\n",
    "        best_idx = similarities[0].argmax().item()\n",
    "        best_class = class_labels[best_idx]\n",
    "        best_score = similarities[0, best_idx].item()\n",
    "        best_probability = probabilities[best_idx].item()\n",
    "        \n",
    "        # Create results dictionary\n",
    "        results = {\n",
    "            'predicted_class': best_class,\n",
    "            'confidence_score': best_score,\n",
    "            'probability': best_probability,\n",
    "            'all_scores': {label: score.item() for label, score in zip(class_labels, similarities[0])},\n",
    "            'all_probabilities': {label: prob.item() for label, prob in zip(class_labels, probabilities)}\n",
    "        }\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def classify_multiple_images(self, image_paths: List[str], class_labels: List[str]) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Classify multiple images against the same set of class labels\n",
    "        \"\"\"\n",
    "        similarities = self.clip_app.predict_similarity(image_paths, class_labels)\n",
    "        \n",
    "        results = []\n",
    "        for i, image_path in enumerate(image_paths):\n",
    "            # Get probabilities for this image\n",
    "            probabilities = F.softmax(similarities[i] / 100.0, dim=0)\n",
    "            \n",
    "            # Get the best match for this image\n",
    "            best_idx = similarities[i].argmax().item()\n",
    "            best_class = class_labels[best_idx]\n",
    "            best_score = similarities[i, best_idx].item()\n",
    "            best_probability = probabilities[best_idx].item()\n",
    "            \n",
    "            # Create results for this image\n",
    "            image_results = {\n",
    "                'image_path': image_path,\n",
    "                'predicted_class': best_class,\n",
    "                'confidence_score': best_score,\n",
    "                'probability': best_probability,\n",
    "                'all_scores': {label: score.item() for label, score in zip(class_labels, similarities[i])},\n",
    "                'all_probabilities': {label: prob.item() for label, prob in zip(class_labels, probabilities)}\n",
    "            }\n",
    "            \n",
    "            results.append(image_results)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def get_top_k_predictions(self, image_path: str, class_labels: List[str], k: int = 3) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Get top-k predictions for a single image\n",
    "        \"\"\"\n",
    "        similarities = self.clip_app.predict_similarity([image_path], class_labels)\n",
    "        probabilities = F.softmax(similarities[0] / 100.0, dim=0)\n",
    "        \n",
    "        # Get top-k indices\n",
    "        top_k_indices = similarities[0].topk(k).indices\n",
    "        \n",
    "        top_predictions = []\n",
    "        for idx in top_k_indices:\n",
    "            idx = idx.item()\n",
    "            top_predictions.append({\n",
    "                'class': class_labels[idx],\n",
    "                'confidence_score': similarities[0, idx].item(),\n",
    "                'probability': probabilities[idx].item()\n",
    "            })\n",
    "        \n",
    "        return top_predictions\n",
    "\n",
    "# Your existing functions (fixed versions)\n",
    "def simple_tokenizer(text: str) -> torch.Tensor:\n",
    "    \"\"\"Simple tokenizer that creates a tensor of shape [1, 77]\"\"\"\n",
    "    tokens = torch.randint(0, 1000, (1, 77))\n",
    "    return tokens\n",
    "\n",
    "def image_preprocessor(image: Image.Image) -> torch.Tensor:\n",
    "    \"\"\"Preprocesses image to tensor of shape [3, 224, 224]\"\"\"\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    if image.mode != 'RGB':\n",
    "        image = image.convert('RGB')\n",
    "    \n",
    "    tensor = transform(image)\n",
    "    return tensor\n",
    "\n",
    "class MockClipModel:\n",
    "    def __call__(self, images: torch.Tensor, texts: torch.Tensor) -> torch.Tensor:\n",
    "        return torch.randn(images.shape[0], texts.shape[0]) * 100\n",
    "\n",
    "# Initialize everything\n",
    "clip_app = ClipApp(\n",
    "    model=MockClipModel(),\n",
    "    text_tokenizer=simple_tokenizer,\n",
    "    image_preprocessor=image_preprocessor\n",
    ")\n",
    "\n",
    "# Create classifier\n",
    "classifier = ClipClassifier(clip_app)\n",
    "\n",
    "def test_classification():\n",
    "    # Define your image and possible classes\n",
    "    image_path = r'C:\\Users\\qc_de\\Documents\\HaQathon\\anythingLLMtests\\screenshot.png'\n",
    "    # image_path = r'C:\\Users\\qc_de\\Documents\\HaQathon\\anythingLLMtests\\chess.png'\n",
    "    # image_path = '/Users/satvik/Documents/GitHub/HaQathon/camera.png'  # Replace with your image path\n",
    "    \n",
    "    # Define your classification labels\n",
    "    class_labels = [\n",
    "        \"code\",\n",
    "        \"games\",\n",
    "        \"video\"\n",
    "    ]\n",
    "    \n",
    "    try:\n",
    "        print(\"=== Single Image Classification ===\")\n",
    "        \n",
    "        # Classify single image\n",
    "        result = classifier.classify_single_image(image_path, class_labels)\n",
    "        \n",
    "        print(f\"Predicted class: {result['predicted_class']}\")\n",
    "        print(f\"Confidence score: {result['confidence_score']:.2f}\")\n",
    "        print(f\"Probability: {result['probability']:.2%}\")\n",
    "        \n",
    "        print(\"\\n=== All Class Scores ===\")\n",
    "        for label, score in result['all_scores'].items():\n",
    "            prob = result['all_probabilities'][label]\n",
    "            print(f\"{label}: {score:.2f} (probability: {prob:.2%})\")\n",
    "        \n",
    "        print(\"\\n=== Top 3 Predictions ===\")\n",
    "        top_3 = classifier.get_top_k_predictions(image_path, class_labels, k=3)\n",
    "        for i, pred in enumerate(top_3, 1):\n",
    "            print(f\"{i}. {pred['class']}: {pred['confidence_score']:.2f} ({pred['probability']:.2%})\")\n",
    "        \n",
    "        # Example with multiple images\n",
    "        print(\"\\n=== Multiple Images Classification ===\")\n",
    "        multiple_images = [image_path]  # Add more image paths here\n",
    "        multiple_results = classifier.classify_multiple_images(multiple_images, class_labels)\n",
    "        \n",
    "        for result in multiple_results:\n",
    "            print(f\"Image: {result['image_path']}\")\n",
    "            print(f\"Predicted: {result['predicted_class']} ({result['probability']:.2%})\")\n",
    "            print()\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error during classification: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_classification()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804e5588",
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    test_classification()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
